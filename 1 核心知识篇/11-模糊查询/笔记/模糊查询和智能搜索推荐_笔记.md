## 模糊查询

- ### 前缀搜索：prefix

   ##### 概念：以xx开头的搜索，不计算相关度评分。

   ##### 注意：

   - 前缀搜索匹配的是term，而不是field。
   - 前缀搜索的性能很差
   - 前缀搜索没有缓存
   - 前缀搜索尽可能把前缀长度设置的更长

   ##### 语法：

   ```json
   GET <index>/_search
   {
     "query": {
       "prefix": {
         "<field>": {
           "value": "<word_prefix>"
         }
       }
     }
   }
   index_prefixes: 默认   "min_chars" : 2,   "max_chars" : 5 
   ```

- ### 通配符：wildcard

   ##### 概念：通配符运算符是匹配一个或多个字符的占位符。例如，*通配符运算符匹配零个或多个字符。您可以将通配符运算符与其他字符结合使用以创建通配符模式。

   ##### 注意：

   - 通配符匹配的也是term，而不是field

   ##### 语法：

   ```json
   GET <index>/_search
   {
     "query": {
       "wildcard": {
         "<field>": {
           "value": "<word_with_wildcard>"
         }
       }
     }
   }
   ```

- ### 正则：regexp

   ##### 概念：regexp查询的性能可以根据提供的正则表达式而有所不同。为了提高性能，应避免使用通配符模式，如.*或 .*?+未经前缀或后缀

   ##### 语法：

   ```json
   GET <index>/_search
   {
     "query": {
       "regexp": {
         "<field>": {
           "value": "<regex>",
           "flags": "ALL",
         }
       }
     }
   }
   ```

   ##### flags

   - ALL

     启用所有可选操作符。

   - COMPLEMENT

     启用~操作符。可以使用~对下面最短的模式进行否定。例如

     a~bc  # matches 'adc' and 'aec' but not 'abc'

   - INTERVAL

     启用<>操作符。可以使用<>匹配数值范围。例如

     foo<1-100>    # matches 'foo1', 'foo2' ... 'foo99', 'foo100'

     foo<01-100>   # matches 'foo01', 'foo02' ... 'foo99', 'foo100'

   - INTERSECTION

     启用&操作符，它充当AND操作符。如果左边和右边的模式都匹配，则匹配成功。例如:

     aaa.+&.+bbb  # matches 'aaabbb'

   - ANYSTRING

     启用@操作符。您可以使用@来匹配任何整个字符串。
     您可以将@操作符与&和~操作符组合起来，创建一个“everything except”逻辑。例如:

     @&~(abc.+)  # matches everything except terms beginning with 'abc'

- ### 模糊查询：fuzzy

   混淆字符 (**b**ox → fox)							缺少字符 (**b**lack → lack)

   多出字符 (sic → sic**k**)							 颠倒次序 (a**c**t → **c**at)

   ##### 语法

   ```json
   GET <index>/_search
   {
     "query": {
       "fuzzy": {
         "<field>": {
           "value": "<keyword>"
         }
       }
     }
   }
   ```

   ##### 参数：

   - value：（必须，关键词）

   - fuzziness：编辑距离，（0，1，2）并非越大越好，召回率高但结果不准确

     1) 两段文本之间的Damerau-Levenshtein距离是使一个字符串与另一个字符串匹配所需的插入、删除、替换和调换的数量

     2) 距离公式：Levenshtein是lucene的，es改进版：Damerau-Levenshtein，

     axe=>aex  Levenshtein=2  Damerau-Levenshtein=1
     
   - transpositions：（可选，布尔值）指示编辑是否包括两个相邻字符的变位（ab→ba）。默认为true。

- ### 短语前缀：match_phrase_prefix

   ##### match_phrase：

   - match_phrase会分词
   - 被检索字段必须包含match_phrase中的所有词项并且顺序必须是相同的
   - 被检索字段包含的match_phrase中的词项之间不能有其他词项

   ##### 概念：

   ​	match_phrase_prefix与match_phrase相同,但是它多了一个特性,就是它允许在文本的最后一个词项(term)上的前缀匹配,如果 是一个单词,比如a,它会匹配文档字段所有以a开头的文档,如果是一个短语,比如 "this is ma" ,他会先在倒排索引中做以ma做前缀搜索,然后在匹配到的doc中做match_phrase查询,(网上有的说是先match_phrase,然后再进行前缀搜索, 是不对的)

   ##### 参数

   - analyzer 指定何种分析器来对该短语进行分词处理
   - max_expansions 限制匹配的最大词项
   - boost 用于设置该查询的权重
   - slop 允许短语间的词项(term)间隔：slop 参数告诉 match_phrase 查询词条相隔多远时仍然能将文档视为匹配 什么是相隔多远？ 意思是说为了让查询和文档匹配你需要移动词条多少次？

   ##### 原理解析：https://www.elastic.co/cn/blog/found-fuzzy-search#performance-considerations

- ### N-gram和edge ngram

   ##### tokenizer

   ```json
   GET _analyze
   {
     "tokenizer": "ngram",
     "text": "reba always loves me"
   }
   ```

   ##### token filter

   ```json
   GET _analyze
   {
     "tokenizer": "ik_max_word",
     "filter": [ "ngram" ],
     "text": "reba always loves me"
   }
   ```

   ##### min_gram：创建索引所拆分字符的最小阈值

   ##### max_gram：创建索引所拆分字符的最大阈值

   ##### ngram：从每一个字符开始,按照步长,进行分词,适合前缀中缀检索

   ##### edge_ngram：从第一个字符开始,按照步长,进行分词,适合前缀匹配场景

   

   

