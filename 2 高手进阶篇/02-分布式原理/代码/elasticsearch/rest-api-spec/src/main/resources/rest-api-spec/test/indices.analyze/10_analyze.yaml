# Will be performed before each test as a part of the test setup
#
setup:
  - do:
      ping: {}

---
"Basic test":
    - do:
        indices.analyze:
          body:
            text: Foo Bar
    - length: { tokens: 2 }
    - match:     { tokens.0.token: foo }
    - match:     { tokens.1.token: bar }

---
"Tokenizer and filter":
    - do:
        indices.analyze:
          body:
            filter:       [lowercase]
            text:         Foo Bar
            tokenizer:    keyword
    - length: { tokens: 1 }
    - match:     { tokens.0.token: foo bar }

---
"Index and field":
    - do:
        indices.create:
          index: test
          body:
            mappings:
              test:
                properties:
                  text:
                    type:     text
                    analyzer: whitespace

    - do:
        indices.analyze:
          index: test
          body:
            field: text
            text: Foo Bar!
    - length: { tokens: 2 }
    - match:     { tokens.0.token: Foo }
    - match:     { tokens.1.token: Bar! }
---
"JSON in Body":
    - do:
        indices.analyze:
          body: { "text": "Foo Bar", "filter": ["lowercase"], "tokenizer": keyword }
    - length: {tokens: 1 }
    - match:     { tokens.0.token: foo bar }
---
"Array text":
    - do:
        indices.analyze:
          body: { "text": ["Foo Bar", "Baz"], "filter": ["lowercase"], "tokenizer": keyword }
    - length: {tokens: 2 }
    - match:     { tokens.0.token: foo bar }
    - match:     { tokens.1.token: baz }
---
"Detail response with Analyzer":
    - do:
        indices.analyze:
          body: {"text": "This is troubled", "analyzer": standard, "explain": "true"}
    - length: { detail.analyzer.tokens: 3 }
    - match:     { detail.analyzer.name: standard }
    - match:     { detail.analyzer.tokens.0.token: this }
    - match:     { detail.analyzer.tokens.1.token: is }
    - match:     { detail.analyzer.tokens.2.token: troubled }
---
"Detail output spcified attribute":
    - do:
        indices.analyze:
          body: {"text": "<text>This is troubled</text>", "char_filter": ["html_strip"], "filter": ["snowball"], "tokenizer": standard, "explain": true, "attributes": ["keyword"]}
    - length: { detail.charfilters: 1 }
    - length: { detail.tokenizer.tokens: 3 }
    - length: { detail.tokenfilters.0.tokens: 3 }
    - match:     { detail.tokenizer.name: standard }
    - match:     { detail.tokenizer.tokens.0.token: This }
    - match:     { detail.tokenizer.tokens.1.token: is }
    - match:     { detail.tokenizer.tokens.2.token: troubled }
    - match:     { detail.tokenfilters.0.name: snowball }
    - match:     { detail.tokenfilters.0.tokens.0.token: This }
    - match:     { detail.tokenfilters.0.tokens.1.token: is }
    - match:     { detail.tokenfilters.0.tokens.2.token: troubl }
    - match:     { detail.tokenfilters.0.tokens.2.keyword: false }

---
"Custom filter in request":
    - do:
        indices.analyze:
          body: { "text": "Foo Bar Buzz", "filter": ["lowercase",   {     "type": "stop", "stopwords": ["foo", "buzz"]}], "tokenizer": whitespace, "explain": true }
    - length: {detail.tokenizer.tokens: 3 }
    - length: {detail.tokenfilters.0.tokens: 3 }
    - length: {detail.tokenfilters.1.tokens: 1 }
    - match:     { detail.tokenizer.name: whitespace }
    - match:     { detail.tokenizer.tokens.0.token: Foo }
    - match:     { detail.tokenizer.tokens.1.token: Bar }
    - match:     { detail.tokenizer.tokens.2.token: Buzz }
    - match:     { detail.tokenfilters.0.name: lowercase }
    - match:     { detail.tokenfilters.0.tokens.0.token: foo }
    - match:     { detail.tokenfilters.0.tokens.1.token: bar }
    - match:     { detail.tokenfilters.0.tokens.2.token: buzz }
    - match:     { detail.tokenfilters.1.name: "_anonymous_tokenfilter_[1]" }
    - match:     { detail.tokenfilters.1.tokens.0.token: bar }
---
"Custom char_filter in request":
    - do:
        indices.analyze:
          body: { "text": "jeff quit phish", "char_filter": [{"type": "mapping", "mappings": ["ph => f", "qu => q"]}], "tokenizer": keyword }
    - length: {tokens: 1 }
    - match:     { tokens.0.token: "jeff qit fish" }

---
"Custom tokenizer in request":
    - do:
        indices.analyze:
          body: { "text": "good", "tokenizer": {"type": "nGram", "min_gram": 2, "max_gram": 2}, "explain": true }
    - length: {detail.tokenizer.tokens: 3 }
    - match:     { detail.tokenizer.name: _anonymous_tokenizer }
    - match:     { detail.tokenizer.tokens.0.token: go }
    - match:     { detail.tokenizer.tokens.1.token: oo }
    - match:     { detail.tokenizer.tokens.2.token: od }
